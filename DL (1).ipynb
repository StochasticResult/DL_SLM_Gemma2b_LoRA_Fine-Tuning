{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "060e054ceef34a09b77b2b1994d60b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f18db0291f34f27aba6dc99b42ebe08",
              "IPY_MODEL_ef806847f0314b06a08909bd48ac367d",
              "IPY_MODEL_637d0688d92f4953b85ba39daefe0fc6"
            ],
            "layout": "IPY_MODEL_4cd547c81c924a04999305915392ac39"
          }
        },
        "1f18db0291f34f27aba6dc99b42ebe08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd8bf8f38f94604b16f8d5565601978",
            "placeholder": "​",
            "style": "IPY_MODEL_023ac9e1d6b84631adddbc5ca50c522b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ef806847f0314b06a08909bd48ac367d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a70084ce5a24bb183936867fbb3abec",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd8536559c9d4c169aa233b9538def00",
            "value": 3
          }
        },
        "637d0688d92f4953b85ba39daefe0fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81112b40f0524a6c9262b5c7a70021b1",
            "placeholder": "​",
            "style": "IPY_MODEL_cc8ad5f0558e438c945ef9b1b168d422",
            "value": " 3/3 [00:42&lt;00:00, 12.07s/it]"
          }
        },
        "4cd547c81c924a04999305915392ac39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd8bf8f38f94604b16f8d5565601978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "023ac9e1d6b84631adddbc5ca50c522b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a70084ce5a24bb183936867fbb3abec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8536559c9d4c169aa233b9538def00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81112b40f0524a6c9262b5c7a70021b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8ad5f0558e438c945ef9b1b168d422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c1aa16283cc4ca89aa86252c19296d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_348a12ca0ff847fa889fd65f9c5ed356",
              "IPY_MODEL_6e9206c5c8a945baa55f434dd882036b",
              "IPY_MODEL_a3e6feb6d94141ff834ca90682286ac4"
            ],
            "layout": "IPY_MODEL_3d9660b179ed46b6a6698fbe230de06a"
          }
        },
        "348a12ca0ff847fa889fd65f9c5ed356": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a7b224fd40d47b6af34953a0d648053",
            "placeholder": "​",
            "style": "IPY_MODEL_1192a3d49a0846bfb96190cb3ce70fce",
            "value": "Downloading readme: "
          }
        },
        "6e9206c5c8a945baa55f434dd882036b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_861e7d948dc1473590c675233a5bae96",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69669a4ce3134530a2a10a37001865a5",
            "value": 1
          }
        },
        "a3e6feb6d94141ff834ca90682286ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_420a3a34ef5b4a5fae2d2606a017436b",
            "placeholder": "​",
            "style": "IPY_MODEL_3a40697d9a3e42af91ee4ee0863d24dc",
            "value": " 8.20k/? [00:00&lt;00:00, 690kB/s]"
          }
        },
        "3d9660b179ed46b6a6698fbe230de06a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7b224fd40d47b6af34953a0d648053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1192a3d49a0846bfb96190cb3ce70fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "861e7d948dc1473590c675233a5bae96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "69669a4ce3134530a2a10a37001865a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "420a3a34ef5b4a5fae2d2606a017436b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a40697d9a3e42af91ee4ee0863d24dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54f0aa803b684c6a84996aab19c302f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a7b563a49734d6b8a629b09336bf514",
              "IPY_MODEL_26d3fb48f6104d09854c948083512728",
              "IPY_MODEL_a46b98550be34336adadcd10d8629cdf"
            ],
            "layout": "IPY_MODEL_7a4d317ec64c47d7b42f6d0b7d912f36"
          }
        },
        "3a7b563a49734d6b8a629b09336bf514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f7f1673641740008c6eb937b5436147",
            "placeholder": "​",
            "style": "IPY_MODEL_e01f1ded7ace49e7816592474c61b22b",
            "value": "Map: 100%"
          }
        },
        "26d3fb48f6104d09854c948083512728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34b27d404b164f34b26399bf5256f556",
            "max": 15011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5cfb02c2ce0440fb333e11e1ac18064",
            "value": 15011
          }
        },
        "a46b98550be34336adadcd10d8629cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_208a6b5a15d842f285e9700506a07e47",
            "placeholder": "​",
            "style": "IPY_MODEL_3ec50cf21a724771b4326f0483ba32bd",
            "value": " 15011/15011 [00:09&lt;00:00, 1749.90 examples/s]"
          }
        },
        "7a4d317ec64c47d7b42f6d0b7d912f36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7f1673641740008c6eb937b5436147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e01f1ded7ace49e7816592474c61b22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34b27d404b164f34b26399bf5256f556": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5cfb02c2ce0440fb333e11e1ac18064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "208a6b5a15d842f285e9700506a07e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec50cf21a724771b4326f0483ba32bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4dff8a4a38b44ee8975929c084d5f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb3b069e6d094a268c95159292d09e5b",
              "IPY_MODEL_e5a59e18f6d247f6a787aa10c5f43f94",
              "IPY_MODEL_02eb1a4c97214ac98618694f68b74910"
            ],
            "layout": "IPY_MODEL_4f71dfca6aaf4150abdb8d73d54e06a3"
          }
        },
        "fb3b069e6d094a268c95159292d09e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e729d403ca241d5a30aaf50b1d5da80",
            "placeholder": "​",
            "style": "IPY_MODEL_cee7d75f7a6a48f4a1156339715f9d5b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "e5a59e18f6d247f6a787aa10c5f43f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5efa0c7d4e9f4a5189030ba14f323ec6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96a368cfd86a4384bcfe89eb23e500b2",
            "value": 3
          }
        },
        "02eb1a4c97214ac98618694f68b74910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_180789a232e54299b5a7ca436d618521",
            "placeholder": "​",
            "style": "IPY_MODEL_21845c9bd50344ac980d2f94f66b1974",
            "value": " 3/3 [00:51&lt;00:00, 14.34s/it]"
          }
        },
        "4f71dfca6aaf4150abdb8d73d54e06a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e729d403ca241d5a30aaf50b1d5da80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cee7d75f7a6a48f4a1156339715f9d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5efa0c7d4e9f4a5189030ba14f323ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a368cfd86a4384bcfe89eb23e500b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "180789a232e54299b5a7ca436d618521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21845c9bd50344ac980d2f94f66b1974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eae42196ecd458db16715c8548a84dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_0690a328833e4d6b96cd466309aa2b4e"
          }
        },
        "0375ca07e6ba430e86dc5a21eefedf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f33b1ed3168042bb88a913b19685092b",
            "placeholder": "​",
            "style": "IPY_MODEL_f558f8fcbf7c4b5ba6ff918fa17d201b",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a260d467a50b4732a1620e12c85b306c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_56860e03facc4757ad71957811bf2895",
            "placeholder": "​",
            "style": "IPY_MODEL_89de22e8707b4be8837eb8aaf0a3c04d",
            "value": ""
          }
        },
        "e9476aee9435453a88402d68a134b83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_45bb2d22b3064aa1b85ff2b7d44a7401",
            "style": "IPY_MODEL_9a9b3d6ee11a457f9cdeaae0d321961f",
            "value": true
          }
        },
        "50a899a2237d4c179a11f537579381f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6e5a2f93f46c4142b21436a97c08d1db",
            "style": "IPY_MODEL_ef585c9a72b34915928eb297af150ed5",
            "tooltip": ""
          }
        },
        "a511a0a6fc7f4f4bb79bdd8c8542020b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05dd77bd74f44e8da35066e1fb24f434",
            "placeholder": "​",
            "style": "IPY_MODEL_6ff45bbedc674f719ed19cff081aa938",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "0690a328833e4d6b96cd466309aa2b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f33b1ed3168042bb88a913b19685092b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f558f8fcbf7c4b5ba6ff918fa17d201b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56860e03facc4757ad71957811bf2895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89de22e8707b4be8837eb8aaf0a3c04d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45bb2d22b3064aa1b85ff2b7d44a7401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a9b3d6ee11a457f9cdeaae0d321961f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e5a2f93f46c4142b21436a97c08d1db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef585c9a72b34915928eb297af150ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "05dd77bd74f44e8da35066e1fb24f434": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff45bbedc674f719ed19cff081aa938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7019c4340c7f4a21a5b26f29581a6bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c98b7f26f3e464fbe45f52936075222",
            "placeholder": "​",
            "style": "IPY_MODEL_2291d77c76bc4e76b6d33efead78d8af",
            "value": "Connecting..."
          }
        },
        "0c98b7f26f3e464fbe45f52936075222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2291d77c76bc4e76b6d33efead78d8af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyzjX89ec45u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "7eae42196ecd458db16715c8548a84dc",
            "0375ca07e6ba430e86dc5a21eefedf7b",
            "a260d467a50b4732a1620e12c85b306c",
            "e9476aee9435453a88402d68a134b83d",
            "50a899a2237d4c179a11f537579381f9",
            "a511a0a6fc7f4f4bb79bdd8c8542020b",
            "0690a328833e4d6b96cd466309aa2b4e",
            "f33b1ed3168042bb88a913b19685092b",
            "f558f8fcbf7c4b5ba6ff918fa17d201b",
            "56860e03facc4757ad71957811bf2895",
            "89de22e8707b4be8837eb8aaf0a3c04d",
            "45bb2d22b3064aa1b85ff2b7d44a7401",
            "9a9b3d6ee11a457f9cdeaae0d321961f",
            "6e5a2f93f46c4142b21436a97c08d1db",
            "ef585c9a72b34915928eb297af150ed5",
            "05dd77bd74f44e8da35066e1fb24f434",
            "6ff45bbedc674f719ed19cff081aa938",
            "7019c4340c7f4a21a5b26f29581a6bbb",
            "0c98b7f26f3e464fbe45f52936075222",
            "2291d77c76bc4e76b6d33efead78d8af"
          ]
        },
        "outputId": "1625097d-85ec-414e-a5e4-d00a7b19c202"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eae42196ecd458db16715c8548a84dc"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LORA"
      ],
      "metadata": {
        "id": "nWQY_86H7VW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0\n",
        "!pip install -q transformers==4.44.0 datasets==2.19.0 accelerate==0.34.0 trl==0.9.6 loralib evaluate==0.4.2 scipy==1.11.4 numpy==1.26.4 sentencepiece protobuf==4.25.3\n",
        "!pip install -q numpy==1.26.4 scipy==1.11.4\n",
        "# restrart runtime after this command from Runtime tab"
      ],
      "metadata": {
        "id": "vwwJCdzsKnQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fef0c19-277c-4317-f735-4bd8c4ff5144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m767.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires scipy>=1.13, but you have scipy 1.11.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, torch, loralib as lora\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Any, List\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "MODEL_NAME   = \"google/gemma-2-2b\"           # fits on Colab Pro GPU with small batch/accum\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR   = \"/content/outputs/gemma2b-loralib-dolly\"\n",
        "RANK         = 16                             # LoRA rank\n",
        "MAX_LEN      = 768\n",
        "EPOCHS       = 1.5\n",
        "BATCH_SIZE   = 1\n",
        "GRAD_ACC     = 8\n",
        "LR           = 2e-4\n",
        "USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------\n",
        "# Load tokenizer & base model\n",
        "# --------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load model without device_map=\"auto\" initially to ensure all weights are materialized on CPU.\n",
        "# This prevents layers from being on a 'meta' device, allowing loralib to inject properly.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        "    # Removed device_map=\"auto\" here\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Using \"eager\" attention implementation as recommended by earlier stderr for Gemma2 models\n",
        "model.config.attn_implementation = \"eager\"\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "# -------------------\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Inject LoRA (loralib) by replacing target Linear layers\n",
        "# --------------------\n",
        "def _wrap_linear_with_lora(parent: nn.Module, child_name: str, child: nn.Module):\n",
        "    # This check is now less likely to trigger if model is fully loaded to CPU\n",
        "    if child.weight.device.type == \"meta\":\n",
        "        print(f\"⚠️ Skipping {child_name}: still on meta device.\")\n",
        "        return\n",
        "\n",
        "    in_f, out_f = child.in_features, child.out_features\n",
        "    new_layer = lora.Linear(\n",
        "        in_features=in_f,\n",
        "        out_features=out_f,\n",
        "        r=RANK,\n",
        "        bias=(child.bias is not None)\n",
        "    )\n",
        "    # Copy pretrained weights to preserve original functionality\n",
        "    with torch.no_grad():\n",
        "        new_layer.weight.copy_(child.weight.data)\n",
        "        if child.bias is not None:\n",
        "            new_layer.bias.copy_(child.bias.data)\n",
        "\n",
        "    setattr(parent, child_name, new_layer)\n",
        "TARGET_LINEAR_NAMES = {\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "}\n",
        "\n",
        "def _inject_lora(module: nn.Module, prefix: str = \"\"):\n",
        "    # For each child, if it's a Linear whose qualified name ends with any of TARGET_LINEAR_NAMES, wrap with lora.Linear\n",
        "    for name, child in list(module.named_children()):\n",
        "        qname = f\"{prefix}.{name}\" if prefix else name\n",
        "        if isinstance(child, nn.Linear) and any(qname.endswith(tn) for tn in TARGET_LINEAR_NAMES):\n",
        "            _wrap_linear_with_lora(module, name, child)\n",
        "        else:\n",
        "            _inject_lora(child, qname)\n",
        "\n",
        "# actually inject\n",
        "_inject_lora(model)\n",
        "\n",
        "# --- Debug: check what Linear layers exist\n",
        "linear_names = [n for n, m in model.named_modules() if isinstance(m, nn.Linear)]\n",
        "print(f\"Total Linear layers: {len(linear_names)}\")\n",
        "print(linear_names[:10])  # print a few to confirm naming\n",
        "\n",
        "# --- Updated target names for Gemma architecture\n",
        "TARGET_LINEAR_NAMES = {\n",
        "    \"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.o_proj\",\n",
        "    \"mlp.gate_proj\", \"mlp.up_proj\", \"mlp.down_proj\"\n",
        "}\n",
        "\n",
        "# --- Re-inject LoRA with corrected matching\n",
        "def _inject_lora_gemma(module: nn.Module, prefix: str = \"\"):\n",
        "    for name, child in list(module.named_children()):\n",
        "        qname = f\"{prefix}.{name}\" if prefix else name\n",
        "        if isinstance(child, nn.Linear) and any(tn in qname for tn in TARGET_LINEAR_NAMES):\n",
        "            _wrap_linear_with_lora(module, name, child)\n",
        "        else:\n",
        "            _inject_lora_gemma(child, qname)\n",
        "\n",
        "_inject_lora_gemma(model)\n",
        "\n",
        "# --- Verify LoRA successfully injected\n",
        "print(\"Injected LoRA layers:\", count_lora_layers(model))\n",
        "\n",
        "# --- Freeze and verify trainables\n",
        "lora.mark_only_lora_as_trainable(model)\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable/1e6:.2f}M / Total: {total/1e6:.2f}M\")\n",
        "\n",
        "assert trainable > 0, \"❌ No LoRA layers are trainable. Injection failed.\"\n",
        "\n",
        "\n",
        "# ---- add this block ----\n",
        "def count_lora_layers(m: nn.Module):\n",
        "    n = 0\n",
        "    for name, mod in m.named_modules():\n",
        "        if isinstance(mod, lora.Linear):\n",
        "            n += 1\n",
        "    return n\n",
        "# -------------------------\n",
        "\n",
        "print(\"Injected LoRA layers:\", count_lora_layers(model))\n",
        "\n",
        "\n",
        "# freeze non-LoRA parameters\n",
        "lora.mark_only_lora_as_trainable(model)\n",
        "\n",
        "# Manually move the model to GPU *after* LoRA injection.\n",
        "# This ensures the model is on the correct device before Trainer takes over.\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "# sanity: how many params are trainable?\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable/1e6:.2f}M / Total: {total/1e6:.2f}M ({100*trainable/total:.2f}%)\")\n",
        "\n",
        "# --------------------\n",
        "# Data: format Dolly into simple prompt-turns (no chat_template needed)\n",
        "# --------------------\n",
        "def _to_str(x):\n",
        "    if isinstance(x, list):\n",
        "        return \" \".join(map(str, x))\n",
        "    elif isinstance(x, str):\n",
        "        return x\n",
        "    return str(x) if x is not None else \"\"\n",
        "\n",
        "def format_dolly(ex: Dict[str, Any]) -> List[str]:\n",
        "    instr = _to_str(ex.get(\"instruction\", \"\")).strip()\n",
        "    ctx   = _to_str(ex.get(\"context\", \"\")).strip()\n",
        "    resp  = _to_str(ex.get(\"response\", \"\")).strip()\n",
        "\n",
        "    prompt = \"You are a helpful assistant.\"\n",
        "    if instr:\n",
        "        prompt = instr if not ctx else f\"{instr}\\n\\nContext: {ctx}\"\n",
        "    elif ctx:\n",
        "        prompt = ctx\n",
        "\n",
        "    text = (\n",
        "        f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>assistant\\n{resp}\\n<end_of_turn>\"\n",
        "    )\n",
        "    return [text]  # TRL requires list of strings\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# --------------------\n",
        "# Trainer (SFTTrainer)\n",
        "# --------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LR,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.0,\n",
        "    bf16=USE_BF16,\n",
        "    fp16=(not USE_BF16),\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data,\n",
        "    args=training_args,\n",
        "    formatting_func=format_dolly,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Max GPU memory (GB):\", torch.cuda.max_memory_allocated() / (1024**3))\n",
        "\n",
        "# --------------------\n",
        "# Save: only LoRA parameters\n",
        "# --------------------\n",
        "LORA_ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"loralib_adapter.pt\")\n",
        "torch.save(lora.lora_state_dict(model), LORA_ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved LoRA adapter to:\", LORA_ADAPTER_PATH)\n",
        "x"
      ],
      "metadata": {
        "id": "DqO3GmpXRwhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506,
          "referenced_widgets": [
            "f4dff8a4a38b44ee8975929c084d5f62",
            "fb3b069e6d094a268c95159292d09e5b",
            "e5a59e18f6d247f6a787aa10c5f43f94",
            "02eb1a4c97214ac98618694f68b74910",
            "4f71dfca6aaf4150abdb8d73d54e06a3",
            "4e729d403ca241d5a30aaf50b1d5da80",
            "cee7d75f7a6a48f4a1156339715f9d5b",
            "5efa0c7d4e9f4a5189030ba14f323ec6",
            "96a368cfd86a4384bcfe89eb23e500b2",
            "180789a232e54299b5a7ca436d618521",
            "21845c9bd50344ac980d2f94f66b1974"
          ]
        },
        "outputId": "35d28f17-188d-4f8d-838e-5aa416ad25f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4dff8a4a38b44ee8975929c084d5f62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Linear layers: 183\n",
            "['model.layers.0.self_attn.q_proj', 'model.layers.0.self_attn.k_proj', 'model.layers.0.self_attn.v_proj', 'model.layers.0.self_attn.o_proj', 'model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.1.self_attn.q_proj', 'model.layers.1.self_attn.k_proj', 'model.layers.1.self_attn.v_proj']\n",
            "Injected LoRA layers: 182\n",
            "Trainable params: 20.77M / Total: 2635.11M\n",
            "Injected LoRA layers: 182\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 82.00 MiB. GPU ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2374618462.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;31m# This ensures the model is on the correct device before Trainer takes over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m# sanity: how many params are trainable?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2882\u001b[0m                 )\n\u001b[0;32m-> 2883\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2885\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     )\n\u001b[0;32m-> 1159\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 82.00 MiB. GPU "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, torch, loralib as lora\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Any, List\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "MODEL_NAME   = \"google/gemma-2-2b\"           # fits on Colab Pro GPU with small batch/accum\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR   = \"/content/outputs/gemma2b-loralib-dolly\"\n",
        "RANK         = 16                             # LoRA rank\n",
        "MAX_LEN      = 768\n",
        "EPOCHS       = 1.5\n",
        "BATCH_SIZE   = 1\n",
        "GRAD_ACC     = 8\n",
        "LR           = 2e-4\n",
        "USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------\n",
        "# Load tokenizer & base model (no quantization)\n",
        "# --------------------\n",
        "# --------------------\n",
        "# Load tokenizer & base model (4-bit quantized)\n",
        "# --------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "\n",
        "model.config.attn_implementation = \"flash_attention_2\"\n",
        "\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "# -------------------\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Inject LoRA (loralib) by replacing target Linear layers\n",
        "# --------------------\n",
        "def _wrap_linear_with_lora(parent: nn.Module, child_name: str, child: nn.Module):\n",
        "    # Skip meta tensors (lazy-loaded weights that aren't materialized yet)\n",
        "    if child.weight.device.type == \"meta\":\n",
        "        print(f\"⚠️ Skipping {child_name}: still on meta device.\")\n",
        "        return\n",
        "\n",
        "    in_f, out_f = child.in_features, child.out_features\n",
        "    new_layer = lora.Linear(\n",
        "        in_features=in_f,\n",
        "        out_features=out_f,\n",
        "        r=RANK,\n",
        "        bias=(child.bias is not None)\n",
        "    )\n",
        "    # Copy pretrained weights to preserve original functionality\n",
        "    with torch.no_grad():\n",
        "        new_layer.weight.copy_(child.weight.data)\n",
        "        if child.bias is not None:\n",
        "            new_layer.bias.copy_(child.bias.data)\n",
        "\n",
        "    setattr(parent, child_name, new_layer)\n",
        "TARGET_LINEAR_NAMES = {\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "}\n",
        "\n",
        "def _inject_lora(module: nn.Module, prefix: str = \"\"):\n",
        "    # For each child, if it's a Linear whose qualified name ends with any of TARGET_LINEAR_NAMES, wrap with lora.Linear\n",
        "    for name, child in list(module.named_children()):\n",
        "        qname = f\"{prefix}.{name}\" if prefix else name\n",
        "        if isinstance(child, nn.Linear) and any(qname.endswith(tn) for tn in TARGET_LINEAR_NAMES):\n",
        "            _wrap_linear_with_lora(module, name, child)\n",
        "        else:\n",
        "            _inject_lora(child, qname)\n",
        "\n",
        "# actually inject\n",
        "# actually inject\n",
        "_inject_lora(model)\n",
        "\n",
        "# ---- add this block ----\n",
        "def count_lora_layers(m: nn.Module):\n",
        "    n = 0\n",
        "    for name, mod in m.named_modules():\n",
        "        if isinstance(mod, lora.Linear):\n",
        "            n += 1\n",
        "    return n\n",
        "# -------------------------\n",
        "\n",
        "print(\"Injected LoRA layers:\", count_lora_layers(model))\n",
        "\n",
        "\n",
        "\n",
        "# freeze non-LoRA parameters\n",
        "lora.mark_only_lora_as_trainable(model)\n",
        "\n",
        "# sanity: how many params are trainable?\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable/1e6:.2f}M / Total: {total/1e6:.2f}M ({100*trainable/total:.2f}%)\")\n",
        "\n",
        "# --------------------\n",
        "# Data: format Dolly into simple prompt-turns (no chat_template needed)\n",
        "# --------------------\n",
        "def _to_str(x):\n",
        "    if isinstance(x, list):\n",
        "        return \" \".join(map(str, x))\n",
        "    elif isinstance(x, str):\n",
        "        return x\n",
        "    return str(x) if x is not None else \"\"\n",
        "\n",
        "def format_dolly(ex: Dict[str, Any]) -> List[str]:\n",
        "    instr = _to_str(ex.get(\"instruction\", \"\")).strip()\n",
        "    ctx   = _to_str(ex.get(\"context\", \"\")).strip()\n",
        "    resp  = _to_str(ex.get(\"response\", \"\")).strip()\n",
        "\n",
        "    prompt = \"You are a helpful assistant.\"\n",
        "    if instr:\n",
        "        prompt = instr if not ctx else f\"{instr}\\n\\nContext: {ctx}\"\n",
        "    elif ctx:\n",
        "        prompt = ctx\n",
        "\n",
        "    text = (\n",
        "        f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>assistant\\n{resp}\\n<end_of_turn>\"\n",
        "    )\n",
        "    return [text]  # TRL requires list of strings\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# --------------------\n",
        "# Trainer (SFTTrainer)\n",
        "# --------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LR,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.0,\n",
        "    bf16=USE_BF16,\n",
        "    fp16=(not USE_BF16),\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data,\n",
        "    args=training_args,\n",
        "    formatting_func=format_dolly,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Max GPU memory (GB):\", torch.cuda.max_memory_allocated() / (1024**3))\n",
        "\n",
        "# --------------------\n",
        "# Save: only LoRA parameters\n",
        "# --------------------\n",
        "LORA_ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"loralib_adapter.pt\")\n",
        "torch.save(lora.lora_state_dict(model), LORA_ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved LoRA adapter to:\", LORA_ADAPTER_PATH)\n"
      ],
      "metadata": {
        "id": "uzf15cLERRKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, torch, loralib as lora\n",
        "import torch.nn as nn\n",
        "from typing import Dict, Any, List\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "MODEL_NAME   = \"google/gemma-2-2b\"           # fits on Colab Pro GPU with small batch/accum\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR   = \"/content/outputs/gemma2b-loralib-dolly\"\n",
        "RANK         = 16                             # LoRA rank\n",
        "MAX_LEN      = 768\n",
        "EPOCHS       = 1.5\n",
        "BATCH_SIZE   = 1\n",
        "GRAD_ACC     = 8\n",
        "LR           = 2e-4\n",
        "USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------\n",
        "# Load tokenizer & base model\n",
        "# --------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Load model without device_map=\"auto\" initially to ensure all weights are materialized on CPU.\n",
        "# This prevents layers from being on a 'meta' device, allowing loralib to inject properly.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        "    # Removed device_map=\"auto\" here\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Using \"eager\" attention implementation as recommended by earlier stderr for Gemma2 models\n",
        "model.config.attn_implementation = \"eager\"\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "# -------------------\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Inject LoRA (loralib) by replacing target Linear layers\n",
        "# --------------------\n",
        "def _wrap_linear_with_lora(parent: nn.Module, child_name: str, child: nn.Module):\n",
        "    # This check is now less likely to trigger if model is fully loaded to CPU\n",
        "    if child.weight.device.type == \"meta\":\n",
        "        print(f\"⚠️ Skipping {child_name}: still on meta device.\")\n",
        "        return\n",
        "\n",
        "    in_f, out_f = child.in_features, child.out_features\n",
        "    new_layer = lora.Linear(\n",
        "        in_features=in_f,\n",
        "        out_features=out_f,\n",
        "        r=RANK,\n",
        "        bias=(child.bias is not None)\n",
        "    )\n",
        "    # Copy pretrained weights to preserve original functionality\n",
        "    with torch.no_grad():\n",
        "        new_layer.weight.copy_(child.weight.data)\n",
        "        if child.bias is not None:\n",
        "            new_layer.bias.copy_(child.bias.data)\n",
        "\n",
        "    setattr(parent, child_name, new_layer)\n",
        "TARGET_LINEAR_NAMES = {\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "}\n",
        "\n",
        "def _inject_lora(module: nn.Module, prefix: str = \"\"):\n",
        "    # For each child, if it's a Linear whose qualified name ends with any of TARGET_LINEAR_NAMES, wrap with lora.Linear\n",
        "    for name, child in list(module.named_children()):\n",
        "        qname = f\"{prefix}.{name}\" if prefix else name\n",
        "        if isinstance(child, nn.Linear) and any(qname.endswith(tn) for tn in TARGET_LINEAR_NAMES):\n",
        "            _wrap_linear_with_lora(module, name, child)\n",
        "        else:\n",
        "            _inject_lora(child, qname)\n",
        "\n",
        "# actually inject\n",
        "_inject_lora(model)\n",
        "\n",
        "# ---- add this block ----\n",
        "def count_lora_layers(m: nn.Module):\n",
        "    n = 0\n",
        "    for name, mod in m.named_modules():\n",
        "        if isinstance(mod, lora.Linear):\n",
        "            n += 1\n",
        "    return n\n",
        "# -------------------------\n",
        "\n",
        "print(\"Injected LoRA layers:\", count_lora_layers(model))\n",
        "\n",
        "\n",
        "# freeze non-LoRA parameters\n",
        "lora.mark_only_lora_as_trainable(model)\n",
        "\n",
        "# Manually move the model to GPU *after* LoRA injection.\n",
        "# This ensures the model is on the correct device before Trainer takes over.\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "# sanity: how many params are trainable?\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable params: {trainable/1e6:.2f}M / Total: {total/1e6:.2f}M ({100*trainable/total:.2f}%\")\n",
        "\n",
        "# --------------------\n",
        "# Data: format Dolly into simple prompt-turns (no chat_template needed)\n",
        "# --------------------\n",
        "def _to_str(x):\n",
        "    if isinstance(x, list):\n",
        "        return \" \".join(map(str, x))\n",
        "    elif isinstance(x, str):\n",
        "        return x\n",
        "    return str(x) if x is not None else \"\"\n",
        "\n",
        "def format_dolly(ex: Dict[str, Any]) -> List[str]:\n",
        "    instr = _to_str(ex.get(\"instruction\", \"\")).strip()\n",
        "    ctx   = _to_str(ex.get(\"context\", \"\")).strip()\n",
        "    resp  = _to_str(ex.get(\"response\", \"\")).strip()\n",
        "\n",
        "    prompt = \"You are a helpful assistant.\"\n",
        "    if instr:\n",
        "        prompt = instr if not ctx else f\"{instr}\\n\\nContext: {ctx}\"\n",
        "    elif ctx:\n",
        "        prompt = ctx\n",
        "\n",
        "    text = (\n",
        "        f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>assistant\\n{resp}\\n<end_of_turn>\"\n",
        "    )\n",
        "    return [text]  # TRL requires list of strings\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# --------------------\n",
        "# Trainer (SFTTrainer)\n",
        "# --------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LR,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.0,\n",
        "    bf16=USE_BF16,\n",
        "    fp16=(not USE_BF16),\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_data,\n",
        "    args=training_args,\n",
        "    formatting_func=format_dolly,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Max GPU memory (GB):\", torch.cuda.max_memory_allocated() / (1024**3))\n",
        "\n",
        "# --------------------\n",
        "# Save: only LoRA parameters\n",
        "# --------------------\n",
        "LORA_ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"loralib_adapter.pt\")\n",
        "torch.save(lora.lora_state_dict(model), LORA_ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved LoRA adapter to:\", LORA_ADAPTER_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "060e054ceef34a09b77b2b1994d60b2f",
            "1f18db0291f34f27aba6dc99b42ebe08",
            "ef806847f0314b06a08909bd48ac367d",
            "637d0688d92f4953b85ba39daefe0fc6",
            "4cd547c81c924a04999305915392ac39",
            "4fd8bf8f38f94604b16f8d5565601978",
            "023ac9e1d6b84631adddbc5ca50c522b",
            "1a70084ce5a24bb183936867fbb3abec",
            "bd8536559c9d4c169aa233b9538def00",
            "81112b40f0524a6c9262b5c7a70021b1",
            "cc8ad5f0558e438c945ef9b1b168d422",
            "2c1aa16283cc4ca89aa86252c19296d4",
            "348a12ca0ff847fa889fd65f9c5ed356",
            "6e9206c5c8a945baa55f434dd882036b",
            "a3e6feb6d94141ff834ca90682286ac4",
            "3d9660b179ed46b6a6698fbe230de06a",
            "3a7b224fd40d47b6af34953a0d648053",
            "1192a3d49a0846bfb96190cb3ce70fce",
            "861e7d948dc1473590c675233a5bae96",
            "69669a4ce3134530a2a10a37001865a5",
            "420a3a34ef5b4a5fae2d2606a017436b",
            "3a40697d9a3e42af91ee4ee0863d24dc",
            "54f0aa803b684c6a84996aab19c302f0",
            "3a7b563a49734d6b8a629b09336bf514",
            "26d3fb48f6104d09854c948083512728",
            "a46b98550be34336adadcd10d8629cdf",
            "7a4d317ec64c47d7b42f6d0b7d912f36",
            "1f7f1673641740008c6eb937b5436147",
            "e01f1ded7ace49e7816592474c61b22b",
            "34b27d404b164f34b26399bf5256f556",
            "a5cfb02c2ce0440fb333e11e1ac18064",
            "208a6b5a15d842f285e9700506a07e47",
            "3ec50cf21a724771b4326f0483ba32bd"
          ]
        },
        "outputId": "8e76b7c3-a732-4590-8966-c576c9198adc",
        "id": "IQGoX_qXQLBV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "060e054ceef34a09b77b2b1994d60b2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "⚠️ Skipping q_proj: still on meta device.\n",
            "⚠️ Skipping k_proj: still on meta device.\n",
            "⚠️ Skipping v_proj: still on meta device.\n",
            "⚠️ Skipping o_proj: still on meta device.\n",
            "⚠️ Skipping gate_proj: still on meta device.\n",
            "⚠️ Skipping up_proj: still on meta device.\n",
            "⚠️ Skipping down_proj: still on meta device.\n",
            "Injected LoRA layers: 77\n",
            "Trainable params: 8.79M / Total: 2623.13M (0.33%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c1aa16283cc4ca89aa86252c19296d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54f0aa803b684c6a84996aab19c302f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched using accelerate hooks.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1521743076.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m trainer = SFTTrainer(\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001b[0m\n\u001b[1;32m    411\u001b[0m             )\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quantization_method\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQuantizationMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBITS_AND_BYTES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         ):\n\u001b[0;32m--> 545\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't move a model that has some modules offloaded to cpu or disk.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pyrhon 3.11"
      ],
      "metadata": {
        "id": "GDwPRT_a7Z0i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfVUzbAuQKoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers==4.44.0 datasets==2.19.0 accelerate==0.34.0 peft==0.10.0 trl==0.9.6\n",
        "!pip install bitsandbytes==0.43.1\n",
        "!pip install numpy==1.26.4 scipy==1.11.4 sentencepiece protobuf==4.25.3\n",
        "\n"
      ],
      "metadata": {
        "id": "3fynvvNJRwp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "from typing import Dict, Any, List\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# === Config ===\n",
        "MODEL_NAME = \"google/gemma-2-2b-it\"  # Safer for T4. You can switch to \"google/gemma-2-2b\" after it's stable\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR   = \"/content/outputs/gemma2-lora-dolly\"\n",
        "MAX_LEN      = 512          # start safe on T4. Try 768/1024 later.\n",
        "EPOCHS       = 1.0\n",
        "BATCH_SIZE   = 1\n",
        "GRAD_ACC     = 8\n",
        "LR           = 2e-4\n",
        "RANK         = 16\n",
        "USE_BF16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# === Tokenizer ===\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "# === 4-bit quantization (bnb) ===\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemExit(\"❌ GPU not found. Switch Colab runtime to GPU (T4/L4/A100).\")\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base.config.pad_token_id = tok.pad_token_id\n",
        "base.config.use_cache = False\n",
        "# Try FA2 if available on your GPU stack\n",
        "try:\n",
        "    base.config.attn_implementation = \"flash_attention_2\"\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# === LoRA via PEFT (no loralib) ===\n",
        "lora_cfg = LoraConfig(\n",
        "    r=RANK,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        ")\n",
        "model = get_peft_model(base, lora_cfg)\n",
        "model.print_trainable_parameters()  # sanity check\n",
        "\n",
        "# === Dolly formatting ===\n",
        "def _to_str(x):\n",
        "    if isinstance(x, list): return \" \".join(map(str, x))\n",
        "    if isinstance(x, str):  return x\n",
        "    return \"\" if x is None else str(x)\n",
        "\n",
        "def format_dolly(ex: Dict[str, Any]) -> List[str]:\n",
        "    instr = _to_str(ex.get(\"instruction\", \"\")).strip()\n",
        "    ctx   = _to_str(ex.get(\"context\", \"\")).strip()\n",
        "    resp  = _to_str(ex.get(\"response\", \"\")).strip()\n",
        "    prompt = instr if not ctx else f\"{instr}\\n\\nContext: {ctx}\"\n",
        "    if not prompt:\n",
        "        prompt = \"You are a helpful assistant.\"\n",
        "    text = (\n",
        "        f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>assistant\\n{resp}\\n<end_of_turn>\"\n",
        "    )\n",
        "    return [text]  # TRL expects list[str]\n",
        "\n",
        "ds = load_dataset(DATASET_NAME)\n",
        "train_ds = ds[\"train\"]\n",
        "\n",
        "# === Trainer ===\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LR,\n",
        "    max_grad_norm=1.0,\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.0,\n",
        "    bf16=USE_BF16,\n",
        "    fp16=(not USE_BF16),\n",
        "    gradient_checkpointing=True,  # memory saver\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tok,\n",
        "    train_dataset=train_ds,\n",
        "    args=args,\n",
        "    formatting_func=format_dolly,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "# quick preview\n",
        "print(\"Preview:\", format_dolly(train_ds[0])[0][:160].replace(\"\\n\",\" ⏎ \"), \"...\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# === Save adapters only ===\n",
        "adapter_dir = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
        "trainer.model.save_pretrained(adapter_dir)\n",
        "tok.save_pretrained(OUTPUT_DIR)\n",
        "print(\"✅ Saved LoRA adapter to:\", adapter_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Fuhh7BxI4zpy",
        "outputId": "35be995f-2edf-43b2-c85d-191798b9f472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1162081014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m from transformers import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.19.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLORA"
      ],
      "metadata": {
        "id": "W4P_LsGeQouS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ug1iBvBHQoCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Gemma-2-2B QLoRA fine-tuning on Dolly-15k  (TRL ≥ 0.25)\n",
        "# Works on Python 3.12 + PyTorch 2.4 + Transformers ≥ 4.45\n",
        "# ==========================================================\n",
        "import os, torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# --------------------\n",
        "# Config\n",
        "# --------------------\n",
        "MODEL_NAME   = \"google/gemma-2-2b\"\n",
        "DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
        "OUTPUT_DIR   = \"/content/outputs/gemma2b-qlora-dolly\"\n",
        "MAX_LEN      = 768\n",
        "EPOCHS       = 1.5\n",
        "BATCH_SIZE   = 1\n",
        "GRAD_ACC     = 8\n",
        "LR           = 2e-4\n",
        "RANK         = 16\n",
        "LORA_ALPHA   = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "USE_BF16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------\n",
        "# Tokenizer\n",
        "# --------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --------------------\n",
        "# Quantization (bitsandbytes)\n",
        "# --------------------\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# Base model\n",
        "# --------------------\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "model.gradient_checkpointing_enable()\n",
        "prepare_model_for_kbit_training(model)\n",
        "\n",
        "# --------------------\n",
        "# PEFT (LoRA)\n",
        "# --------------------\n",
        "target_modules = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "]\n",
        "peft_config = LoraConfig(\n",
        "    r=RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=target_modules,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# --------------------\n",
        "# Dataset preprocessing\n",
        "# --------------------\n",
        "def format_dolly(ex):\n",
        "    instr = (ex.get(\"instruction\") or \"\").strip()\n",
        "    ctx   = (ex.get(\"context\") or \"\").strip()\n",
        "    resp  = (ex.get(\"response\") or \"\").strip()\n",
        "    prompt = \"You are a helpful assistant.\"\n",
        "    if instr:\n",
        "        prompt = instr if not ctx else f\"{instr}\\n\\nContext: {ctx}\"\n",
        "    elif ctx:\n",
        "        prompt = ctx\n",
        "    text = (\n",
        "        f\"<start_of_turn>user\\n{prompt}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>assistant\\n{resp}\\n<end_of_turn>\"\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train_data = dataset[\"train\"].map(format_dolly, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "tokenized_train = train_data.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# --------------------\n",
        "# TrainingArguments\n",
        "# --------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LR,\n",
        "    max_grad_norm=1.0,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    bf16=USE_BF16,\n",
        "    fp16=(not USE_BF16),\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# Trainer (TRL 0.25+)\n",
        "# --------------------\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        ")\n",
        "\n",
        "# --------------------\n",
        "# Train\n",
        "# --------------------\n",
        "trainer.train()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "    print(\"Max GPU memory (GB):\", torch.cuda.max_memory_allocated() / (1024**3))\n",
        "\n",
        "# --------------------\n",
        "# Save Adapter + Tokenizer\n",
        "# --------------------\n",
        "trainer.model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"✅ QLoRA adapter + tokenizer saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826,
          "referenced_widgets": [
            "6c97fc0f0b1a4c85b891a1c2d4d95045",
            "aa7675fd7da9494cbbd43268aa893c6e",
            "6d9d927d9b294c4b8d92594394991f89",
            "9d5f92acd6484bf183f3936aa3b5af83",
            "74527e6eada647f9b88373eaa26d7d35",
            "83759630647d46b2b1b671251b36f525",
            "119ba36063fc4d70918741189e18f332",
            "15f336c7546e486f8956e27b76519a86",
            "c1f8fb5c71704360b28aebae9f8c7324",
            "044df5d04e0a4f40aea1cd89978c149b",
            "17164d57b9fd4994921f124805f098de",
            "df27130b6c1c474b969839623ce8f42e",
            "dcb743b430c847378b38c0e5facb7661",
            "9ec3217d1a1b4d6ea2d028352e2ffd8c",
            "f5f067d5ebdf424692857f3329eb7854",
            "72335414c316403098457a75e05ad77a",
            "19aabc64b41942d9ab9d0b016778da82",
            "c7e3108cac3e44f586b82a7d5123dab3",
            "ca25b4a331f643e0adddd18e6fea1d91",
            "a005031e771745bba896330b25e03421",
            "43040ab7fbb044d08880f46da1bbb836",
            "73f7b6b24a9b4502bd1e1f9ff17d4add",
            "12394acc970745f59dbc709dbf287312",
            "f4f5639dfb6a462c860e49347badb068",
            "a38531f5866b4a7b9648be116bedb0f4",
            "bd135bec42d548da97ea1202826a09d9",
            "caf2ae1af82e4b698faf676fa7077458",
            "66cc876960ae4dffb1939b92d9993384",
            "339f10a8db9746418531b4c33df6e576",
            "521fe6eabaae4a12aa3750210872e05f",
            "696884fb91f94e74baef564707db314b",
            "a052ae266752434bb5fd651c78a8d6f0",
            "8f9ae46a9ed54a9e9524ccb028935ca5",
            "5769d318282c4c00a5810fd34d576c82",
            "022fc9e572094feda9b1466a497df791",
            "edcffbe591a443d6a98f4cacff2c046d",
            "4499dcdf776d46d2ab94f77e13b56985",
            "015c4fef8dfc474b93fe2590682de7ad",
            "6d04e0b6f1754378854bb497dec2d19f",
            "6c35bf730f7142b89a98c40d1a4e9f45",
            "cb6be5df26624510b748cddc3cded8ad",
            "8e8128b046dd472291f5ea7c7fc24d96",
            "4c2d39d1a94d46b2a26e11317bd4735e",
            "baa117290e0a4d5f8e5a531765cfd02a"
          ]
        },
        "id": "3sM0xcnU9qeR",
        "outputId": "ba3eb877-ee45-496a-a20f-25c08122835f"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c97fc0f0b1a4c85b891a1c2d4d95045",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df27130b6c1c474b969839623ce8f42e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12394acc970745f59dbc709dbf287312",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5769d318282c4c00a5810fd34d576c82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='86' max='2816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  86/2816 1:04:10 < 34:45:49, 0.02 it/s, Epoch 0.05/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>18.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.608800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.520100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.999400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.791900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.758500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='107' max='2816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 107/2816 1:20:13 < 34:29:36, 0.02 it/s, Epoch 0.06/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>18.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.608800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.520100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.115100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.999400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.791900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.758500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.848500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2FPQ1R3Knase"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to image stable diffusion model"
      ],
      "metadata": {
        "id": "kf71MpSAnctc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "jcq6e72SnisI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y sentence-transformers gcsfs\n",
        "!pip install -q diffusers==0.27.2 transformers==4.40.1 accelerate==0.29.3 peft==0.10.0 bitsandbytes>=0.45.0 datasets==2.19.0 torchvision huggingface_hub==0.25.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji0NOLjPnowq",
        "outputId": "d4e23ba8-90d1-4d41-d9b5-cf82056be360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping sentence-transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gcsfs as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restart Runtime after install\n"
      ],
      "metadata": {
        "id": "2m61NmYixiP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change Param 'Use_QLora' in configuration to decide bnb_config to be Lora or QLora"
      ],
      "metadata": {
        "id": "eEQxzbgXp-sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from diffusers import DDPMScheduler, UNet2DConditionModel, AutoencoderKL\n",
        "from transformers import CLIPTextModel, CLIPTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# CONFIGURATION\n",
        "USE_QLORA      = True   # <--- SET TO FALSE for Standard LoRA\n",
        "MODEL_ID       = \"runwayml/stable-diffusion-v1-5\" # Base SD 1.5 model\n",
        "DATASET_ID     = \"alexnasa/vangogh\"               # Van Gogh Dataset\n",
        "OUTPUT_DIR     = \"./sd-vangogh-lora\" if not USE_QLORA else \"./sd-vangogh-qlora\"\n",
        "RESOLUTION     = 512\n",
        "TRAIN_BATCH    = 1\n",
        "GRAD_ACCUM     = 4\n",
        "LEARNING_RATE  = 1e-4\n",
        "MAX_STEPS      = 100    # Short run for profiling (increase for real results)\n",
        "CHECKPOINTING  = True\n",
        "\n",
        "\n",
        "# PROFILING UTILITIES\n",
        "class Profiler:\n",
        "    def __init__(self):\n",
        "        self.start_time = None\n",
        "        self.step_times = []\n",
        "\n",
        "    def start(self):\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def step_end(self):\n",
        "        self.step_times.append(time.time())\n",
        "\n",
        "    def print_stats(self, step):\n",
        "        # Calculate memory\n",
        "        mem_alloc = torch.cuda.memory_allocated() / 1024**3\n",
        "        mem_max   = torch.cuda.max_memory_allocated() / 1024**3\n",
        "        mem_res   = torch.cuda.memory_reserved() / 1024**3\n",
        "\n",
        "        # Calculate speed\n",
        "        if len(self.step_times) > 1:\n",
        "            avg_time = (self.step_times[-1] - self.step_times[0]) / len(self.step_times)\n",
        "            speed = 1.0 / avg_time\n",
        "        else:\n",
        "            speed = 0.0\n",
        "\n",
        "        print(f\"[Step {step}] VRAM: {mem_max:.2f}GB (Peak) | Speed: {speed:.2f} it/s\")\n",
        "        return mem_max\n",
        "\n",
        "\n",
        "# 1. SETUP & MODEL LOADING\n",
        "accelerator = Accelerator(gradient_accumulation_steps=GRAD_ACCUM, mixed_precision=\"bf16\")\n",
        "profiler = Profiler()\n",
        "\n",
        "# Load Scheduler & Tokenizer\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
        "\n",
        "# Load VAE and Text Encoder (Frozen, usually loaded in 16-bit to save memory)\n",
        "vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Load UNet (The core model we finetune)\n",
        "print(f\"Loading UNet... Mode: {'QLoRA (4-bit)' if USE_QLORA else 'LoRA (16-bit)'}\")\n",
        "\n",
        "if USE_QLORA:\n",
        "    # QLoRA: Load in 4-bit NF4\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\", quantization_config=bnb_config)\n",
        "    unet = prepare_model_for_kbit_training(unet) # Important for gradient checkpointing in 4-bit\n",
        "else:\n",
        "    # Standard LoRA: Load in Float16\n",
        "    unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\", torch_dtype=torch.bfloat16)\n",
        "\n",
        "unet.enable_gradient_checkpointing() # Save memory\n",
        "\n",
        "\n",
        "# 2. INJECT LORA ADAPTERS\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"], # Standard SD targets\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "unet = get_peft_model(unet, lora_config)\n",
        "unet.print_trainable_parameters()\n",
        "\n",
        "# Move models to GPU (Accelerator handles UNet, we manually move frozen ones)\n",
        "vae.to(accelerator.device)\n",
        "text_encoder.to(accelerator.device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# 3. DATASET PREP\n",
        "dataset = load_dataset(DATASET_ID, split=\"train\")\n",
        "\n",
        "# Transform: Image -> Tensor, Text -> Token IDs\n",
        "def transform_fn(examples):\n",
        "    # 1. Process Images\n",
        "    images = [transforms.Resize((RESOLUTION, RESOLUTION))(img).convert(\"RGB\") for img in examples[\"image\"]]\n",
        "    pixel_values = [transforms.ToTensor()(img) * 2.0 - 1.0 for img in images] # List of Tensors\n",
        "\n",
        "    # 2. Process Captions\n",
        "    inputs = tokenizer(\n",
        "        examples[\"caption\"],\n",
        "        max_length=tokenizer.model_max_length,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # 3. CRITICAL FIX: Return ONLY the tensors.\n",
        "    # Do not return 'image' (PIL) or 'caption' (String) keys.\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": inputs.input_ids\n",
        "    }\n",
        "\n",
        "train_dataset = dataset.with_transform(transform_fn)\n",
        "\n",
        "# Now the DataLoader will only see Tensors, resolving the TypeError\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH, shuffle=True)\n",
        "\n",
        "# Prepare with Accelerator\n",
        "unet, optimizer, train_dataloader = accelerator.prepare(unet, optimizer, train_dataloader)\n",
        "\n",
        "\n",
        "# 4. TRAINING LOOP\n",
        "print(\"\\nStarting Training...\")\n",
        "profiler.start()\n",
        "\n",
        "unet.train()\n",
        "global_step = 0\n",
        "\n",
        "while global_step < MAX_STEPS:\n",
        "    for batch in train_dataloader:\n",
        "        with accelerator.accumulate(unet):\n",
        "            # A. Convert images to latents (using VAE)\n",
        "            # We must cast to weight_dtype (fp16) because VAE is frozen in fp16\n",
        "            latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.bfloat16, device=accelerator.device)).latent_dist.sample()\n",
        "            latents = latents * vae.config.scaling_factor\n",
        "\n",
        "            # B. Add Noise\n",
        "            noise = torch.randn_like(latents)\n",
        "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # C. Get Text Embeddings\n",
        "            encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(accelerator.device))[0]\n",
        "\n",
        "            # D. Predict Noise (Forward Pass)\n",
        "            # For QLoRA, input must match compute dtype (fp16), but weights are 4-bit\n",
        "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "\n",
        "            # E. Loss & Backward\n",
        "            loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if accelerator.sync_gradients:\n",
        "            global_step += 1\n",
        "            profiler.step_end()\n",
        "\n",
        "            if global_step % 10 == 0:\n",
        "                peak_mem = profiler.print_stats(global_step)\n",
        "                print(f\"   Loss: {loss.item():.4f}\")\n",
        "\n",
        "            if global_step >= MAX_STEPS:\n",
        "                break\n",
        "\n",
        "\n",
        "# 5. SAVE ADAPTERS\n",
        "if accelerator.is_main_process:\n",
        "    unet = accelerator.unwrap_model(unet)\n",
        "    unet.save_pretrained(OUTPUT_DIR)\n",
        "    print(f\"\\nSaved LoRA adapters to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh8686vln5dg",
        "outputId": "045fda16-451c-46ed-b00c-a4c056573f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading UNet... Mode: QLoRA (4-bit)\n",
            "trainable params: 3,188,736 || all params: 862,709,700 || trainable%: 0.3696186561945461\n",
            "\n",
            "Starting Training...\n",
            "[Step 10] VRAM: 6.08GB (Peak) | Speed: 0.13 it/s\n",
            "   Loss: 0.4792\n",
            "[Step 20] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.3079\n",
            "[Step 30] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.0065\n",
            "[Step 40] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.2232\n",
            "[Step 50] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.0169\n",
            "[Step 60] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.0840\n",
            "[Step 70] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.3208\n",
            "[Step 80] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.2421\n",
            "[Step 90] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.0134\n",
            "[Step 100] VRAM: 6.08GB (Peak) | Speed: 0.12 it/s\n",
            "   Loss: 0.1641\n",
            "\n",
            "Saved LoRA adapters to ./sd-vangogh-qlora\n"
          ]
        }
      ]
    }
  ]
}